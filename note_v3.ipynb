{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 24.02.24\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv(\"set/train.csv\")\n",
    "test = pd.read_csv(\"set/test.csv\")\n",
    "sample_sub = pd.read_csv(\"set/sample_solution.csv\", index_col=\"Id\")\n",
    "\n",
    "# Preprocess data\n",
    "X_train = np.array(train.iloc[:, 2:]).reshape(-1, 28, 28, 1).astype(np.float32) / 255.0\n",
    "X_test = np.array(test.iloc[:, 1:]).reshape(-1, 28, 28, 1).astype(np.float32) / 255.0\n",
    "y_col = pd.get_dummies(train['Categoria']).columns  \n",
    "y_train = pd.get_dummies(train['Categoria']).to_numpy()\n",
    "\n",
    "# Split data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), padding='same', activation='relu', kernel_initializer='he_normal', input_shape=(28, 28, 1)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(32, (3, 3), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2), padding='same'),\n",
    "    layers.Dropout(0.25),\n",
    "    \n",
    "    layers.Conv2D(64, (3, 3), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(64, (3, 3), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2), padding='same'),\n",
    "    layers.Dropout(0.3),\n",
    "    \n",
    "    layers.Conv2D(128, (3, 3), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(128, (3, 3), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2), padding='same'),\n",
    "    layers.Dropout(0.4),            # --> BEST: 0.9395\n",
    "\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(256, activation='relu', kernel_initializer='he_normal'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), padding='same', activation='relu', kernel_initializer='he_normal', input_shape=(28, 28, 1)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(32, (3, 3), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2), padding='same'),\n",
    "    layers.Dropout(0.25),\n",
    "    \n",
    "    layers.Conv2D(64, (3, 3), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(64, (3, 3), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2), padding='same'),\n",
    "    layers.Dropout(0.32),\n",
    "    \n",
    "    layers.Conv2D(128, (3, 3), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(128, (3, 3), padding='same', activation='relu', kernel_initializer='he_normal'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2), padding='same'),\n",
    "    layers.Dropout(0.42),       # BEST -> 9422\n",
    "\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(256, activation='relu', kernel_initializer='he_normal'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.51),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "lr_scheduler = LearningRateScheduler(lambda epoch, lr: lr * 0.9 if epoch % 10 == 0 and epoch != 0 else lr)\n",
    "model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', save_best_only=True)\n",
    "\n",
    "# Train model\n",
    "#history = model.fit(X_train, y_train, batch_size=64, epochs=50, validation_data=(X_val, y_val), callbacks=[early_stopping, lr_scheduler, model_checkpoint])\n",
    "history = model.fit(X_train, y_train, batch_size=64, epochs=50, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "#model.load_weights('best_model.h5')\n",
    "\n",
    "# Predictions\n",
    "prediction = model.predict(X_test)\n",
    "pred_df = pd.DataFrame(prediction,columns=y_col,index=sample_sub.index)\n",
    "prediction_col = pred_df.idxmax(axis=1)\n",
    "sample_sub.Categoria = prediction_col\n",
    "\n",
    "sample_sub.to_csv(\"submission_2.csv\",index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
